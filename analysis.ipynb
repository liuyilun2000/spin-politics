{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Load the DataFrame\n",
    "WORK_DIR = \"/home/hpc/b207dd/b207dd11/test/spin-politics\"\n",
    "\n",
    "model_name = \"llama3-8b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_ACTIVATIONS_CACHE_DIR = f\"/home/atuin/b207dd/b207dd11/test/DEU/standard_sentences/{model_name}\"\n",
    "\n",
    "\n",
    "# df.to_pickle(f\"{STANDARD_ACTIVATIONS_CACHE_DIR}/standard_text_with_embeddings.pkl\")\n",
    "\n",
    "df = pd.read_pickle(f\"{STANDARD_ACTIVATIONS_CACHE_DIR}/standard_text_with_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to convert categorical values to RGB values\n",
    "def category_to_rgb(libertarian, collectivist, progressive):\n",
    "    color_map = {\n",
    "        \"Neutral\": 128,\n",
    "        \"Libertär\": 225,\n",
    "        \"Restriktiv\": 30,\n",
    "        \"Kollektivistisch\": 225,\n",
    "        \"Individualistisch\": 30,\n",
    "        \"Progressiv\": 225,\n",
    "        \"Konservativ\": 30\n",
    "    }\n",
    "    r = color_map[libertarian]\n",
    "    g = color_map[collectivist]\n",
    "    b = color_map[progressive]\n",
    "    return f'rgb({r},{g},{b})'\n",
    "\n",
    "df['Ideology'] = df['Libertarian'] + ' - ' + df['Collectivist'] + ' - ' + df['Progressive']\n",
    "df['Color'] = df.apply(lambda row: category_to_rgb(row['Libertarian'], row['Collectivist'], row['Progressive']), axis=1)\n",
    "# Create a color discrete map\n",
    "unique_ideologies = df['Ideology'].unique()\n",
    "color_discrete_map = {ideology: color for ideology, color in zip(unique_ideologies, df['Color'].unique())}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised PCA + UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pca_umap_plot(df, \n",
    "                  pca_cumul_var_ratio_thresh=None,\n",
    "                  pca_n_components=None, \n",
    "                  umap_n_components=3, \n",
    "                  umap_n_neighbors=15, \n",
    "                  umap_min_dist=0.1, \n",
    "                  layer_list=None,\n",
    "                  image_output_folder=\"\"):\n",
    "    embeddings = np.stack(df['Embedding'].values)\n",
    "    n_layers = embeddings.shape[1]\n",
    "    if layer_list is None:\n",
    "        layer_list = range(n_layers)\n",
    "    #\n",
    "    for layer in tqdm(layer_list, desc=\"Processing layers\"):\n",
    "        layer_embeddings = embeddings[:, layer, :]\n",
    "        # Standardize embeddings\n",
    "        mean = np.mean(layer_embeddings, axis=0)\n",
    "        std = np.std(layer_embeddings, axis=0)\n",
    "        standardized_embeddings = (layer_embeddings - mean) / std\n",
    "        # PCA\n",
    "        if pca_n_components:\n",
    "            num_components = pca_n_components\n",
    "            pca = PCA(n_components=num_components)\n",
    "            pca_embeddings = pca.fit_transform(standardized_embeddings)\n",
    "        elif pca_cumul_var_ratio_thresh:\n",
    "            pca = PCA()\n",
    "            pca.fit(standardized_embeddings)\n",
    "            cumul_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "            num_components = np.where(cumul_var_ratio >= pca_cumul_var_ratio_thresh)[0][0] + 1\n",
    "            if num_components < umap_n_components:\n",
    "                num_components = umap_n_components\n",
    "            pca_embeddings = pca.transform(standardized_embeddings)[:, :num_components]\n",
    "        print(f\"Layer {layer} PCA num_components {num_components}\")\n",
    "        # UMAP\n",
    "        if umap_n_components < num_components:\n",
    "            umap_reducer = umap.UMAP(n_components=umap_n_components, \n",
    "                                    n_neighbors=umap_n_neighbors, \n",
    "                                    min_dist=umap_min_dist, \n",
    "                                    metric='cosine', \n",
    "                                    random_state=42)\n",
    "            umap_embeddings = umap_reducer.fit_transform(pca_embeddings)\n",
    "        else:\n",
    "            umap_embeddings = pca_embeddings\n",
    "        # df for plotting\n",
    "        plot_df = df.copy()\n",
    "        plot_df['UMAP1'] = umap_embeddings[:, 0]\n",
    "        plot_df['UMAP2'] = umap_embeddings[:, 1]\n",
    "        plot_df['UMAP3'] = umap_embeddings[:, 2] if umap_n_components == 3 else np.zeros(umap_embeddings.shape[0])\n",
    "        # Plotting\n",
    "        fig = px.scatter_3d(\n",
    "            plot_df, x='UMAP1', y='UMAP2', z='UMAP3', \n",
    "            color='Ideology', \n",
    "            color_discrete_map=color_discrete_map,\n",
    "            #color_continuous_scale=px.colors.sequential.Rainbow_r,\n",
    "            #color_discrete_sequence=px.colors.sequential.Rainbow_r,\n",
    "            title=f'Layer {layer} Embeddings',\n",
    "            hover_data={\n",
    "                'Libertarian': True,\n",
    "                'Collectivist': True,\n",
    "                'Progressive': True,\n",
    "                'Topic': True,\n",
    "                #'Response': True\n",
    "            }\n",
    "        )\n",
    "        file_path = f'{image_output_folder}/scatter_layer{layer}_pca{num_components}_umap{umap_n_components}.html'\n",
    "        fig.write_html(file_path)\n",
    "        plt.close()\n",
    "        plt.clf()\n",
    "\n",
    "# Call the function\n",
    "pca_umap_plot(\n",
    "    df, \n",
    "    #pca_cumul_var_ratio_thresh=.5,\n",
    "    pca_n_components=3, \n",
    "    umap_n_components=3, \n",
    "    umap_n_neighbors=100, \n",
    "    umap_min_dist=0.8, \n",
    "    #layer_list=[i for i in range(3)],\n",
    "    image_output_folder=f\"{WORK_DIR}/standard_img\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised (Assigned) Axis Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standard = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value_threshold = 5e-6\n",
    "\n",
    "# Function to calculate mean embeddings and filter significant features\n",
    "def calculate_mean_embeddings_with_filter(df, axis, p_value_threshold=p_value_threshold):\n",
    "    type_a_label = axis[\"type_a\"]\n",
    "    type_b_label = axis[\"type_b\"]\n",
    "    #\n",
    "    type_a_embeddings = np.stack(df[df[axis[\"column\"]] == type_a_label]['Embedding'].values)\n",
    "    type_b_embeddings = np.stack(df[df[axis[\"column\"]] == type_b_label]['Embedding'].values)\n",
    "    print(type_a_embeddings.shape, type_b_embeddings.shape)\n",
    "    # Calculate mean and variance\n",
    "    type_a_mean = np.mean(type_a_embeddings, axis=0)\n",
    "    type_b_mean = np.mean(type_b_embeddings, axis=0)\n",
    "    #\n",
    "    type_a_var = np.var(type_a_embeddings, axis=0)\n",
    "    type_b_var = np.var(type_b_embeddings, axis=0)\n",
    "    # Perform t-test for each feature\n",
    "    t_values, p_values = ttest_ind(type_a_embeddings, type_b_embeddings, axis=0, equal_var=False)\n",
    "    # Create filter for significant features\n",
    "    significant_features = p_values < p_value_threshold\n",
    "    # Calculate direction vector using only significant features\n",
    "    direction_vector = np.zeros_like(type_a_mean)\n",
    "    direction_vector[significant_features] = type_a_mean[significant_features] - type_b_mean[significant_features]\n",
    "    # Normalize the direction vector\n",
    "    #norm = np.linalg.norm(direction_vector)\n",
    "    #if norm != 0:\n",
    "    #    direction_vector /= norm    \n",
    "    return direction_vector, significant_features\n",
    "\n",
    "# Define the axes\n",
    "axes = {\n",
    "    \"Libertarian\": {\"column\": \"Libertarian\", \"type_a\": \"Libertär\", \"type_b\": \"Restriktiv\"},\n",
    "    \"Collectivist\": {\"column\": \"Collectivist\", \"type_a\": \"Kollektivistisch\", \"type_b\": \"Individualistisch\"},\n",
    "    \"Progressive\": {\"column\": \"Progressive\", \"type_a\": \"Progressiv\", \"type_b\": \"Konservativ\"}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Calculate mean embeddings and filters for each axis\n",
    "mean_embeddings = {}\n",
    "significant_features = {}\n",
    "for axis_name, axis in axes.items():\n",
    "    mean_embeddings[axis_name], significant_features[axis_name] = calculate_mean_embeddings_with_filter(df_standard, axis)\n",
    "    num_significant_features = np.sum(significant_features[axis_name])\n",
    "    total_features = significant_features[axis_name].size\n",
    "    print(f\"{axis_name}: {num_significant_features} of {total_features} features used\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new DataFrame for plotting\n",
    "plot_data = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "    embedding = row['Embedding']    \n",
    "    # Apply the significant feature filter to the embeddings\n",
    "    filtered_embedding = {}\n",
    "    for axis_name in axes.keys():\n",
    "        filtered_embedding[axis_name] = embedding * significant_features[axis_name]\n",
    "    # Calculate scores for each axis using only significant features\n",
    "    libertarian_score = np.dot(filtered_embedding[\"Libertarian\"].flatten(), mean_embeddings[\"Libertarian\"].flatten())\n",
    "    collectivist_score = np.dot(filtered_embedding[\"Collectivist\"].flatten(), mean_embeddings[\"Collectivist\"].flatten())\n",
    "    progressive_score = np.dot(filtered_embedding[\"Progressive\"].flatten(), mean_embeddings[\"Progressive\"].flatten())\n",
    "    #\n",
    "    plot_data.append({\n",
    "        'Libertarian': row['Libertarian'],\n",
    "        'Collectivist': row['Collectivist'],\n",
    "        'Progressive': row['Progressive'],\n",
    "        'Topic': row['Topic'],\n",
    "        \"Libertarian\": libertarian_score,\n",
    "        \"Collectivist\": collectivist_score,\n",
    "        \"Progressive\": progressive_score,\n",
    "        \"Ideology\": row[\"Ideology\"]\n",
    "    })\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "\n",
    "# Calculate the min and max values for each axis across all layers\n",
    "x_min, x_max = plot_df['Libertarian'].min(), plot_df['Libertarian'].max()\n",
    "y_min, y_max = plot_df['Collectivist'].min(), plot_df['Collectivist'].max()\n",
    "z_min, z_max = plot_df['Progressive'].min(), plot_df['Progressive'].max()\n",
    "\n",
    "# Add a small margin to ensure all points are visible\n",
    "margin = 0.1  # 10% margin\n",
    "x_range = x_max - x_min\n",
    "y_range = y_max - y_min\n",
    "z_range = z_max - z_min\n",
    "# Plotting\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    plot_df, x='Libertarian', y='Collectivist', z='Progressive', \n",
    "    color='Ideology', \n",
    "    color_discrete_map=color_discrete_map,\n",
    "    opacity=0.8,\n",
    "    #color_continuous_scale=px.colors.sequential.Rainbow_r,\n",
    "    #color_discrete_sequence=px.colors.sequential.Rainbow_r,\n",
    "    title=f'Assigned Axes Embeddings (All layers Combined)',\n",
    "    hover_data={\n",
    "        'Libertarian': True,\n",
    "        'Collectivist': True,\n",
    "        'Progressive': True,\n",
    "        'Topic': True,\n",
    "        #'Response': True\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    scene = dict(\n",
    "        xaxis_title='Libertarian',\n",
    "        yaxis_title='Collectivist',\n",
    "        zaxis_title='Progressive',\n",
    "        xaxis=dict(range=[x_min - margin * x_range, x_max + margin * x_range]),\n",
    "        yaxis=dict(range=[y_min - margin * y_range, y_max + margin * y_range]),\n",
    "        zaxis=dict(range=[z_min - margin * z_range, z_max + margin * z_range])\n",
    "    ),\n",
    "    scene_aspectmode='cube'\n",
    ")\n",
    "\n",
    "\n",
    "image_output_folder=f\"{WORK_DIR}/standard_img\"\n",
    "file_path = f'{image_output_folder}/scatter_proj.html'\n",
    "fig.write_html(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### layer-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value_threshold = 5e-6\n",
    "\n",
    "# Function to calculate mean embeddings and filter significant features layer-wise\n",
    "def calculate_mean_embeddings_with_filter(df, axis, p_value_threshold=p_value_threshold):\n",
    "    type_a_label = axis[\"type_a\"]\n",
    "    type_b_label = axis[\"type_b\"]\n",
    "    \n",
    "    type_a_embeddings = np.stack(df[df[axis[\"column\"]] == type_a_label]['Embedding'].values)\n",
    "    type_b_embeddings = np.stack(df[df[axis[\"column\"]] == type_b_label]['Embedding'].values)\n",
    "    print(type_a_embeddings.shape, type_b_embeddings.shape)\n",
    "    \n",
    "    num_layers = type_a_embeddings.shape[1]\n",
    "    direction_vector = np.zeros_like(type_a_embeddings[0])\n",
    "    significant_features = np.zeros_like(type_a_embeddings[0], dtype=bool)\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        # Calculate mean and variance for this layer\n",
    "        type_a_mean = np.mean(type_a_embeddings[:, layer, :], axis=0)\n",
    "        type_b_mean = np.mean(type_b_embeddings[:, layer, :], axis=0)\n",
    "        # Perform t-test for each feature in this layer\n",
    "        t_values, p_values = ttest_ind(type_a_embeddings[:, layer, :], type_b_embeddings[:, layer, :], axis=0, equal_var=False)\n",
    "        # Create filter for significant features in this layer\n",
    "        layer_significant_features = p_values < p_value_threshold\n",
    "        # Calculate direction vector for significant features in this layer\n",
    "        layer_direction_vector = np.zeros_like(type_a_mean)\n",
    "        layer_direction_vector[layer_significant_features] = type_a_mean[layer_significant_features] - type_b_mean[layer_significant_features]\n",
    "        # Store results for this layer\n",
    "        direction_vector[layer, :] = layer_direction_vector\n",
    "        significant_features[layer, :] = layer_significant_features\n",
    "    return direction_vector, significant_features\n",
    "\n",
    "\n",
    "axes = {\n",
    "    \"Libertarian\": {\"column\": \"Libertarian\", \"type_a\": \"Libertär\", \"type_b\": \"Restriktiv\"},\n",
    "    \"Collectivist\": {\"column\": \"Collectivist\", \"type_a\": \"Kollektivistisch\", \"type_b\": \"Individualistisch\"},\n",
    "    \"Progressive\": {\"column\": \"Progressive\", \"type_a\": \"Progressiv\", \"type_b\": \"Konservativ\"}\n",
    "}\n",
    "\n",
    "# Calculate mean embeddings and filters for each axis\n",
    "mean_embeddings = {}\n",
    "significant_features = {}\n",
    "for axis_name, axis in axes.items():\n",
    "    mean_embeddings[axis_name], significant_features[axis_name] = calculate_mean_embeddings_with_filter(df_standard, axis)\n",
    "    # Print summary for each layer\n",
    "    num_layers = significant_features[axis_name].shape[0]\n",
    "    for layer in range(num_layers):\n",
    "        num_significant_features = np.sum(significant_features[axis_name][layer])\n",
    "        total_features = significant_features[axis_name][layer].size\n",
    "        print(f\"{axis_name} - Layer {layer}: {num_significant_features} of {total_features} features used\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_layers = 32  # As each embedding has 32 layers\n",
    "\n",
    "# Create a new DataFrame for plotting\n",
    "plot_data = []\n",
    "\n",
    "for layer in tqdm(range(num_layers), desc=\"Processing layers\"):\n",
    "    layer_plot_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        embedding = row['Embedding'][layer]  # Get the embedding for this layer\n",
    "        # Apply the significant feature filter to the embeddings\n",
    "        filtered_embedding = {}\n",
    "        for axis_name in axes.keys():\n",
    "            filtered_embedding[axis_name] = embedding * significant_features[axis_name][layer]\n",
    "        # Calculate scores for each axis using only significant features\n",
    "        libertarian_score = np.dot(filtered_embedding[\"Libertarian\"].flatten(), \n",
    "                                   mean_embeddings[\"Libertarian\"][layer].flatten())\n",
    "        collectivist_score = np.dot(filtered_embedding[\"Collectivist\"].flatten(), \n",
    "                                    mean_embeddings[\"Collectivist\"][layer].flatten())\n",
    "        progressive_score = np.dot(filtered_embedding[\"Progressive\"].flatten(), \n",
    "                                   mean_embeddings[\"Progressive\"][layer].flatten())\n",
    "        layer_plot_data.append({\n",
    "            'Libertarian': libertarian_score,\n",
    "            'Collectivist': collectivist_score,\n",
    "            'Progressive': progressive_score,\n",
    "            'Topic': row['Topic'],\n",
    "            'Ideology': row['Ideology'],\n",
    "            'Layer': layer\n",
    "        })\n",
    "    # Convert layer_plot_data to DataFrame\n",
    "    layer_df = pd.DataFrame(layer_plot_data)\n",
    "    # Min-max normalize scores for this layer\n",
    "    for axis in ['Libertarian', 'Collectivist', 'Progressive']:\n",
    "        min_val = layer_df[axis].min()\n",
    "        max_val = layer_df[axis].max()\n",
    "        layer_df[axis] = (layer_df[axis] - min_val) / (max_val - min_val)\n",
    "    plot_data.extend(layer_df.to_dict('records'))\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Calculate the min and max values for each axis across all layers\n",
    "x_min, x_max = plot_df['Libertarian'].min(), plot_df['Libertarian'].max()\n",
    "y_min, y_max = plot_df['Collectivist'].min(), plot_df['Collectivist'].max()\n",
    "z_min, z_max = plot_df['Progressive'].min(), plot_df['Progressive'].max()\n",
    "\n",
    "# Add a small margin to ensure all points are visible\n",
    "margin = 0.1  # 10% margin\n",
    "x_range = x_max - x_min\n",
    "y_range = y_max - y_min\n",
    "z_range = z_max - z_min\n",
    "\n",
    "# Create 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    plot_df, x='Libertarian', y='Collectivist', z='Progressive', \n",
    "    color='Ideology', \n",
    "    color_discrete_map=color_discrete_map,\n",
    "    opacity=0.8,\n",
    "    title='Assigned Axes Embeddings (Animated through Layers)',\n",
    "    hover_data=['Topic'],\n",
    "    animation_frame='Layer'\n",
    ")\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    scene = dict(\n",
    "        xaxis_title='Libertarian',\n",
    "        yaxis_title='Collectivist',\n",
    "        zaxis_title='Progressive',\n",
    "        xaxis=dict(range=[x_min - margin * x_range, x_max + margin * x_range]),\n",
    "        yaxis=dict(range=[y_min - margin * y_range, y_max + margin * y_range]),\n",
    "        zaxis=dict(range=[z_min - margin * z_range, z_max + margin * z_range])\n",
    "    ),\n",
    "    scene_aspectmode='cube'\n",
    ")\n",
    "\n",
    "# Save the plot\n",
    "image_output_folder = f\"{WORK_DIR}/standard_img\"\n",
    "file_path = f'{image_output_folder}/scatter_proj_animated.html'\n",
    "fig.write_html(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_speech_filtered_concat = pd.read_pickle('df_speech_filtered_concat.pkl')\n",
    "\n",
    "\n",
    "\n",
    "# Function to load embeddings from a tensor file\n",
    "def load_embedding(index, path):\n",
    "    file_path = f\"{path}/{index}.pt\"\n",
    "    return torch.load(file_path, map_location='cpu')\n",
    "\n",
    "# Load all real text activations and add them to the DataFrame\n",
    "ACTIVATIONS_CACHE_DIR = f\"/home/atuin/b207dd/b207dd11/test/DEU/activations/{model_name}\"\n",
    "real_text_embeddings = []\n",
    "\n",
    "for index in tqdm(range(len(df_speech_filtered_concat))):\n",
    "    real_text_embeddings.append(load_embedding(index, ACTIVATIONS_CACHE_DIR).numpy())\n",
    "\n",
    "df_speech_filtered_concat['Embedding'] = real_text_embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_speech_filtered_concat.to_pickle(f\"{ACTIVATIONS_CACHE_DIR}/df_speech_filtered_concat.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ACTIVATIONS_CACHE_DIR = f\"/home/atuin/b207dd/b207dd11/test/DEU/activations/{model_name}\"\n",
    "df_speech_filtered_concat = pd.read_pickle(f\"{ACTIVATIONS_CACHE_DIR}/df_speech_filtered_concat.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_embedding(embeddings):\n",
    "    return np.mean(np.stack(embeddings), axis=0)\n",
    "\n",
    "\n",
    "# Group by 'Speaker' and calculate mean embedding and count of entries\n",
    "speaker_data = []\n",
    "\n",
    "for speaker, group in tqdm(df_speech_filtered_concat.groupby('Speaker')):\n",
    "    mean_embedding = calculate_mean_embedding(group['Embedding'].values)\n",
    "    if np.isnan(mean_embedding).any():\n",
    "        print(group)\n",
    "    speaker_info = group.iloc[0][['Speaker', 'Partei', 'Religion']]\n",
    "    entry_count = len(group)\n",
    "    speaker_data.append({\n",
    "        **speaker_info,\n",
    "        \"Embedding\": mean_embedding,\n",
    "        \"EntryCount\": entry_count\n",
    "    })\n",
    "\n",
    "# Create a new DataFrame from the speaker data\n",
    "df_speaker_mean_embeddings = pd.DataFrame(speaker_data)\n",
    "\n",
    "# Save the new DataFrame to a pickle file\n",
    "df_speaker_mean_embeddings.to_pickle(f\"{ACTIVATIONS_CACHE_DIR}/df_speaker_mean_embeddings.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speaker_mean_embeddings = pd.read_pickle(f\"{ACTIVATIONS_CACHE_DIR}/df_speaker_mean_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting\n",
    "party_color_discrete_map = {\n",
    "    'SPD': '#E3000F',          # Red\n",
    "    'BÜNDNIS 90/DIE GRÜNEN': '#46962B',  # Green\n",
    "    'CSU': '#008AC5',          # Blue\n",
    "    'CDU': '#000000',          # Black\n",
    "    'AfD': '#009EE0',          # Light Blue\n",
    "    'FDP': '#FFED00',          # Yellow\n",
    "    'DIE LINKE.': '#BE3075'    # Magenta\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pca_umap_plot(df, \n",
    "                  pca_cumul_var_ratio_thresh=None,\n",
    "                  pca_n_components=None, \n",
    "                  umap_n_components=3, \n",
    "                  umap_n_neighbors=15, \n",
    "                  umap_min_dist=0.1, \n",
    "                  layer_list=None,\n",
    "                  image_output_folder=\"\"):\n",
    "    embeddings = np.stack(df['Embedding'].values)\n",
    "    n_layers = embeddings.shape[1]\n",
    "    if layer_list is None:\n",
    "        layer_list = range(n_layers)\n",
    "    #\n",
    "    for layer in tqdm(layer_list, desc=\"Processing layers\"):\n",
    "        layer_embeddings = embeddings[:, layer, :]\n",
    "        # Standardize embeddings\n",
    "        mean = np.mean(layer_embeddings, axis=0)\n",
    "        std = np.std(layer_embeddings, axis=0)\n",
    "        standardized_embeddings = (layer_embeddings - mean) / std\n",
    "        # PCA\n",
    "        if pca_n_components:\n",
    "            num_components = pca_n_components\n",
    "            pca = PCA(n_components=num_components)\n",
    "            pca_embeddings = pca.fit_transform(standardized_embeddings)\n",
    "        elif pca_cumul_var_ratio_thresh:\n",
    "            pca = PCA()\n",
    "            pca.fit(standardized_embeddings)\n",
    "            cumul_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "            num_components = np.where(cumul_var_ratio >= pca_cumul_var_ratio_thresh)[0][0] + 1\n",
    "            if num_components < umap_n_components:\n",
    "                num_components = umap_n_components\n",
    "            pca_embeddings = pca.transform(standardized_embeddings)[:, :num_components]\n",
    "        print(f\"Layer {layer} PCA num_components {num_components}\")\n",
    "        # UMAP\n",
    "        if umap_n_components < num_components:\n",
    "            umap_reducer = umap.UMAP(n_components=umap_n_components, \n",
    "                                    n_neighbors=umap_n_neighbors, \n",
    "                                    min_dist=umap_min_dist, \n",
    "                                    metric='cosine', \n",
    "                                    random_state=42)\n",
    "            umap_embeddings = umap_reducer.fit_transform(pca_embeddings)\n",
    "        else:\n",
    "            umap_embeddings = pca_embeddings\n",
    "        # df for plotting\n",
    "        plot_df = df.copy()\n",
    "        plot_df['UMAP1'] = umap_embeddings[:, 0]\n",
    "        plot_df['UMAP2'] = umap_embeddings[:, 1]\n",
    "        plot_df['UMAP3'] = umap_embeddings[:, 2] if umap_n_components == 3 else np.zeros(umap_embeddings.shape[0])\n",
    "        # Plotting\n",
    "        fig = px.scatter_3d(\n",
    "            plot_df, x='UMAP1', y='UMAP2', z='UMAP3', \n",
    "            color='Partei', \n",
    "            size='EntryCount',\n",
    "            opacity=0.5,\n",
    "            color_discrete_map=party_color_discrete_map,\n",
    "            #color_continuous_scale=px.colors.sequential.Rainbow_r,\n",
    "            #color_discrete_sequence=px.colors.sequential.Rainbow_r,\n",
    "            title=f'Layer {layer} Real Speaker Activations on UMAP Axes',\n",
    "            hover_data={\n",
    "                'Speaker': True\n",
    "            }\n",
    "        )\n",
    "        #fig.update_traces(marker_size = 5)\n",
    "        file_path = f'{image_output_folder}/scatter_layer{layer}_pca{num_components}_umap{umap_n_components}.html'\n",
    "        fig.write_html(file_path)\n",
    "        plt.close()\n",
    "        plt.clf()\n",
    "\n",
    "# Call the function\n",
    "pca_umap_plot(\n",
    "    #df_speech_filtered_concat.sample(1000), \n",
    "    df_speaker_mean_embeddings.sort_values(by='EntryCount', ascending=False).iloc[6:],\n",
    "    #pca_cumul_var_ratio_thresh=.5,\n",
    "    pca_n_components=6, \n",
    "    umap_n_components=3, \n",
    "    umap_n_neighbors=20, \n",
    "    umap_min_dist=0.1, \n",
    "    #layer_list=[i for i in range(3)],\n",
    "    #image_output_folder=f\"{WORK_DIR}/real_1000_img\"\n",
    "    image_output_folder=f\"{WORK_DIR}/real_speaker_img\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for plotting\n",
    "plot_data = []\n",
    "\n",
    "for index, row in tqdm(df_speech_filtered_concat.iterrows(), total=df_speech_filtered_concat.shape[0], desc=\"Processing rows\"):\n",
    "    embedding = row['Embedding']\n",
    "    libertarian_score = np.dot(embedding.flatten(), mean_embeddings[\"Libertarian\"].flatten()) - np.dot(embedding.flatten(), mean_embeddings[\"Libertarian\"][1].flatten())\n",
    "    collectivist_score = np.dot(embedding.flatten(), mean_embeddings[\"Collectivist\"].flatten()) - np.dot(embedding.flatten(), mean_embeddings[\"Collectivist\"][1].flatten())\n",
    "    progressive_score = np.dot(embedding.flatten(), mean_embeddings[\"Progressive\"].flatten()) - np.dot(embedding.flatten(), mean_embeddings[\"Progressive\"][1].flatten())\n",
    "    #\n",
    "    plot_data.append({\n",
    "        \"Partei\": row[\"Partei\"],\n",
    "        \"Libertarian\": libertarian_score,\n",
    "        \"Collectivist\": collectivist_score,\n",
    "        \"Progressive\": progressive_score,\n",
    "        \"Text\": row[\"Text\"],\n",
    "    })\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    plot_df, x='Libertarian', y='Collectivist', z='Progressive', \n",
    "    color='Partei', \n",
    "    opacity=0.5,\n",
    "    color_discrete_map=party_color_discrete_map,\n",
    "    #color_continuous_scale=px.colors.sequential.Rainbow_r,\n",
    "    #color_discrete_sequence=px.colors.sequential.Rainbow_r,\n",
    "    title=f'Real Text Activations on Assigned Axes'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_output_folder=f\"{WORK_DIR}/standard_img\"\n",
    "file_path = f'{image_output_folder}/scatter_real_data_manual.html'\n",
    "fig.write_html(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_speaker_mean_embeddings.sort_values(by='EntryCount', ascending=False).iloc[6:]\n",
    "# Create a new DataFrame for plotting\n",
    "plot_data = []\n",
    "\n",
    "for index, row in tqdm(df_tmp.iterrows(), total=df_tmp.shape[0], desc=\"Processing rows\"):\n",
    "    embedding = row['Embedding']\n",
    "    # Apply the significant feature filter to the embeddings\n",
    "    filtered_embedding = {}\n",
    "    for axis_name in axes.keys():\n",
    "        filtered_embedding[axis_name] = embedding * significant_features[axis_name]\n",
    "    # Calculate scores for each axis using only significant features\n",
    "    libertarian_score = np.dot(filtered_embedding[\"Libertarian\"].flatten(), mean_embeddings[\"Libertarian\"].flatten())\n",
    "    collectivist_score = np.dot(filtered_embedding[\"Collectivist\"].flatten(), mean_embeddings[\"Collectivist\"].flatten())\n",
    "    progressive_score = np.dot(filtered_embedding[\"Progressive\"].flatten(), mean_embeddings[\"Progressive\"].flatten())\n",
    "    #\n",
    "    plot_data.append({\n",
    "        \"Speaker\": row[\"Speaker\"],\n",
    "        \"Party\": row[\"Partei\"],\n",
    "        \"Libertarian\": libertarian_score,\n",
    "        \"Collectivist\": collectivist_score,\n",
    "        \"Progressive\": progressive_score,\n",
    "        \"EntryCount\": row[\"EntryCount\"],\n",
    "        \"Category\": 'Speaker'\n",
    "    })\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "\n",
    "# Calculate centroids for each party\n",
    "centroid_data = plot_df.groupby('Party').agg({\n",
    "    'Libertarian': 'mean',\n",
    "    'Collectivist': 'mean',\n",
    "    'Progressive': 'mean',\n",
    "    'EntryCount': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "#centroid_data['Color'] = centroid_data['Party']  # Add a placeholder for Speaker\n",
    "centroid_data['Category'] = 'Party'\n",
    "\n",
    "# Append centroids to plot_df\n",
    "plot_df = pd.concat([plot_df, centroid_data], ignore_index=True)\n",
    "\n",
    "\n",
    "# Calculate the mean and standard deviation for each dimension\n",
    "mean = plot_df[['Libertarian', 'Collectivist', 'Progressive']].mean()\n",
    "std = plot_df[['Libertarian', 'Collectivist', 'Progressive']].std()\n",
    "\n",
    "# Normalize the data (z-score normalization)\n",
    "plot_df['Libertarian'] = (plot_df['Libertarian'] - mean['Libertarian']) / std['Libertarian']\n",
    "plot_df['Collectivist'] = (plot_df['Collectivist'] - mean['Collectivist']) / std['Collectivist']\n",
    "plot_df['Progressive'] = (plot_df['Progressive'] - mean['Progressive']) / std['Progressive']\n",
    "\n",
    "# If you want to scale the data to a specific range, e.g., [-1, 1], you can do:\n",
    "# (This step is optional and depends on your preference)\n",
    "scale_factor = 3  # This will make about 99.7% of the data fall within [-1, 1]\n",
    "plot_df['Libertarian'] /= scale_factor\n",
    "plot_df['Collectivist'] /= scale_factor\n",
    "plot_df['Progressive'] /= scale_factor\n",
    "\n",
    "\n",
    "libertarian_range  = [-0.99, 0.99]\n",
    "collectivist_range = [-0.99, 0.99]\n",
    "progressive_range  = [-0.99, 0.99]\n",
    "\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    plot_df, x='Libertarian', y='Collectivist', z='Progressive', \n",
    "    color='Party', \n",
    "    size='EntryCount',\n",
    "    size_max=64,\n",
    "    symbol='Category',\n",
    "    symbol_map={'Party':'circle', 'Speaker':'square'},\n",
    "    opacity=0.56,\n",
    "    color_discrete_map=party_color_discrete_map,\n",
    "    range_x=libertarian_range,\n",
    "    range_y=collectivist_range,\n",
    "    range_z=progressive_range,\n",
    "    # title=f'Speaker and Party Activations Projected on Assigned Axes (All Layers Combined)',\n",
    "    hover_data={\n",
    "        'Speaker': True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    font_family=\"Helvetica\",\n",
    "    scene = dict(\n",
    "        xaxis_title='Libertarian',\n",
    "        yaxis_title='Collectivist',\n",
    "        zaxis_title='Progressive',\n",
    "    ),\n",
    "    scene_aspectmode='cube',\n",
    "    legend=dict(\n",
    "        orientation=\"h\"\n",
    "    )\n",
    ")\n",
    "\n",
    "image_output_folder=f\"{WORK_DIR}/real_speaker_img\"\n",
    "file_path = f'{image_output_folder}/scatter_real_data_proj.html'\n",
    "fig.write_html(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### layer-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_speaker_mean_embeddings.sort_values(by='EntryCount', ascending=False).iloc[6:]\n",
    "# Create a new DataFrame for plotting\n",
    "plot_data = []\n",
    "\n",
    "num_layers = df_tmp.iloc[0]['Embedding'].shape[0]\n",
    "\n",
    "# Create a new DataFrame for plotting\n",
    "plot_data = []\n",
    "\n",
    "for layer in tqdm(range(num_layers), desc=\"Processing layers\"):\n",
    "    layer_plot_data = []\n",
    "    for index, row in df_tmp.iterrows():\n",
    "        embedding = row['Embedding'][layer]  # Get the embedding for this layer\n",
    "        # Apply the significant feature filter to the embeddings\n",
    "        filtered_embedding = {}\n",
    "        for axis_name in axes.keys():\n",
    "            filtered_embedding[axis_name] = embedding * significant_features[axis_name][layer]\n",
    "        # Calculate scores for each axis using only significant features\n",
    "        libertarian_score = np.dot(filtered_embedding[\"Libertarian\"], mean_embeddings[\"Libertarian\"][layer])\n",
    "        collectivist_score = np.dot(filtered_embedding[\"Collectivist\"], mean_embeddings[\"Collectivist\"][layer])\n",
    "        progressive_score = np.dot(filtered_embedding[\"Progressive\"], mean_embeddings[\"Progressive\"][layer])\n",
    "        #\n",
    "        layer_plot_data.append({\n",
    "            \"Speaker\": row[\"Speaker\"],\n",
    "            \"Partei\": row[\"Partei\"],\n",
    "            \"Libertarian\": libertarian_score,\n",
    "            \"Collectivist\": collectivist_score,\n",
    "            \"Progressive\": progressive_score,\n",
    "            \"EntryCount\": row[\"EntryCount\"],\n",
    "            \"Color\": row[\"Partei\"],\n",
    "            \"Symbol\": 'Speaker'\n",
    "            \"Layer\": layer\n",
    "        })\n",
    "    # Calculate centroids for each party\n",
    "    layer_df = pd.DataFrame(layer_plot_data)\n",
    "    # Min-max normalize scores for this layer\n",
    "    for axis in ['Libertarian', 'Collectivist', 'Progressive']:\n",
    "        min_val = layer_df[axis].min()\n",
    "        max_val = layer_df[axis].max()\n",
    "        layer_df[axis] = (layer_df[axis] - min_val) / (max_val - min_val)\n",
    "    # Calculate centroids after normalization\n",
    "    centroid_data = layer_df.groupby('Partei').agg({\n",
    "        'Libertarian': 'mean',\n",
    "        'Collectivist': 'mean',\n",
    "        'Progressive': 'mean',\n",
    "        'EntryCount': 'sum'\n",
    "    }).reset_index()\n",
    "    centroid_data['Speaker'] = centroid_data['Partei']  # Add a placeholder for Speaker\n",
    "    centroid_data['Symbol'] = 'Partei'\n",
    "    centroid_data['Layer'] = layer\n",
    "    # Append centroids to plot_data\n",
    "    plot_data.extend(layer_df.to_dict('records'))\n",
    "    plot_data.extend(centroid_data.to_dict('records'))\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "\n",
    "# Calculate the min and max values for each axis across all layers\n",
    "libertarian_range = [plot_df['Libertarian'].min(), plot_df['Libertarian'].max()]\n",
    "collectivist_range = [plot_df['Collectivist'].min(), plot_df['Collectivist'].max()]\n",
    "progressive_range = [plot_df['Progressive'].min(), plot_df['Progressive'].max()]\n",
    "\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    plot_df, x='Libertarian', y='Collectivist', z='Progressive', \n",
    "    color='Partei', \n",
    "    size='EntryCount',\n",
    "    size_max=64,\n",
    "    symbol='Symbol',\n",
    "    symbol_map={'Partei':'circle', 'Speaker':'square'},\n",
    "    opacity=0.56,\n",
    "    color_discrete_map=party_color_discrete_map,\n",
    "    range_x=libertarian_range,\n",
    "    range_y=collectivist_range,\n",
    "    range_z=progressive_range,\n",
    "    title='Real Speaker Activations Projected on Assigned Axes (Animated through Layers)',\n",
    "    hover_data=['Speaker'],\n",
    "    animation_frame='Layer'\n",
    ")\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    scene = dict(\n",
    "        xaxis_title='Libertarian',\n",
    "        yaxis_title='Collectivist',\n",
    "        zaxis_title='Progressive',\n",
    "    ),\n",
    "    scene_aspectmode='cube'\n",
    ")\n",
    "\n",
    "\n",
    "image_output_folder=f\"{WORK_DIR}/real_speaker_img\"\n",
    "file_path = f'{image_output_folder}/scatter_real_data_proj_animated.html'\n",
    "fig.write_html(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_speaker_mean_embeddings.sort_values(by='EntryCount', ascending=False).iloc[6:]\n",
    "\n",
    "num_layers = df_tmp.iloc[0]['Embedding'].shape[0]\n",
    "\n",
    "# Create a new DataFrame for plotting\n",
    "plot_data = []\n",
    "\n",
    "for layer in tqdm(range(num_layers)):\n",
    "    layer_plot_data = []\n",
    "    # Prepare data for LDA\n",
    "    X = np.array([row['Embedding'][layer].flatten() for _, row in df_tmp.iterrows()])\n",
    "    y = df_tmp['Partei'].values\n",
    "    # Encode party labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    # Perform LDA\n",
    "    lda = LinearDiscriminantAnalysis(n_components=3)\n",
    "    X_lda = lda.fit_transform(X, y_encoded)\n",
    "    # Create plot data\n",
    "    for i, (_, row) in enumerate(df_tmp.iterrows()):\n",
    "        layer_plot_data.append({\n",
    "            \"Partei\": row[\"Partei\"],\n",
    "            \"LDA1\": X_lda[i, 0],\n",
    "            \"LDA2\": X_lda[i, 1],\n",
    "            \"LDA3\": X_lda[i, 2],\n",
    "            \"Speaker\": row[\"Speaker\"],\n",
    "            \"SpeakerStatus\": row[\"SpeakerStatus\"],\n",
    "            \"Religion\": row[\"Religion\"],\n",
    "            \"EntryCount\": row[\"EntryCount\"],\n",
    "            \"Symbol\": 'Speaker',\n",
    "            \"Layer\": layer\n",
    "        })\n",
    "    # Calculate centroids for each party\n",
    "    centroid_data = pd.DataFrame(layer_plot_data).groupby('Partei').agg({\n",
    "        'LDA1': 'mean',\n",
    "        'LDA2': 'mean',\n",
    "        'LDA3': 'mean',\n",
    "        'EntryCount': 'sum'\n",
    "    }).reset_index()\n",
    "    centroid_data['Speaker'] = centroid_data['Partei']  # Add a placeholder for Speaker\n",
    "    centroid_data['Symbol'] = 'Partei'\n",
    "    centroid_data['Layer'] = layer\n",
    "    # Append centroids to plot_df\n",
    "    plot_data.extend(layer_plot_data)\n",
    "    plot_data.extend(centroid_data.to_dict('records'))\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Calculate the min and max values for each axis across all layers\n",
    "x_min, x_max = plot_df['LDA1'].min(), plot_df['LDA1'].max()\n",
    "y_min, y_max = plot_df['LDA2'].min(), plot_df['LDA2'].max()\n",
    "z_min, z_max = plot_df['LDA3'].min(), plot_df['LDA3'].max()\n",
    "# Add a small margin to ensure all points are visible\n",
    "margin = 0.1  # 10% margin\n",
    "x_range = x_max - x_min\n",
    "y_range = y_max - y_min\n",
    "z_range = z_max - z_min\n",
    "\n",
    "# Create 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    plot_df, x='LDA1', y='LDA2', z='LDA3', \n",
    "    color='Partei', \n",
    "    size='EntryCount',\n",
    "    size_max=64,\n",
    "    symbol='Symbol',\n",
    "    symbol_map={'Partei':'circle', 'Speaker':'square'},\n",
    "    opacity=0.56,\n",
    "    color_discrete_map=party_color_discrete_map,\n",
    "    title='Speaker Embeddings: LDA 3D Projection (Animated through Layers)',\n",
    "    hover_data=['Speaker', 'SpeakerStatus', 'Religion', 'EntryCount'],\n",
    "    animation_frame='Layer'\n",
    ")\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    scene = dict(\n",
    "        xaxis_title='LDA1',\n",
    "        yaxis_title='LDA2',\n",
    "        zaxis_title='LDA3',\n",
    "        xaxis=dict(range=[x_min - margin * x_range, x_max + margin * x_range]),\n",
    "        yaxis=dict(range=[y_min - margin * y_range, y_max + margin * y_range]),\n",
    "        zaxis=dict(range=[z_min - margin * z_range, z_max + margin * z_range])\n",
    "    ),\n",
    "    scene_aspectmode='cube'\n",
    ")\n",
    "\n",
    "\n",
    "# Save the plot\n",
    "image_output_folder = f\"{WORK_DIR}/real_speaker_img\"\n",
    "file_path = f'{image_output_folder}/scatter_lda_3d_animated.html'\n",
    "fig.write_html(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
